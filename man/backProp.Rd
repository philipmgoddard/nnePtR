% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/backward-propogate.R
\name{backProp}
\alias{backProp}
\title{backProp performs a forward propogation with current values
of Thetas, then performs a backward propogation to calculate
gradient needed for optimisation}
\usage{
backProp(unrollThetas, Thetas, nUnits, nLayers, lambda, outcome, a, z, gradient,
  delta, Deltas)
}
\arguments{
\item{unrollThetas}{vector comprised of unrolled parameter matrix elements}

\item{Thetas}{template list of matrices allocated to correct size}

\item{nUnits}{number of units in each hidden layer of network}

\item{nLayers}{number of hidden layers in the network}

\item{lambda}{penalty term}

\item{outcome}{matrix of 'dummied' outcomes}

\item{a}{template list of activation matrices (with "zeroth" layer filled in with inputs)}

\item{z}{template list of transformed activation matrices}

\item{gradient}{template list of matrices of gradients}

\item{delta}{template list of matrices of errors}

\item{Deltas}{template list of matrices of product of activations and errors}
}
\description{
backProp performs a forward propogation with current values
of Thetas, then performs a backward propogation to calculate
gradient needed for optimisation
}

