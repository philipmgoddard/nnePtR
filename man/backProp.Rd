% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/backward-propogate.R
\name{backProp}
\alias{backProp}
\title{backProp performs a forward propogation with current values
of Thetas, then performs a backward propogation to calculate
gradient needed for optimisation}
\usage{
backProp(unrollThetas, Thetas, nUnits, nLayers, lambda, outcome, a)
}
\arguments{
\item{unrollThetas}{vector comprised of unrolled parameter matrix elements}

\item{Thetas}{template list of matrices allocated to correct size}

\item{nUnits}{number of units in each hidden layer of network}

\item{nLayers}{number of hidden layers in the network}

\item{lambda}{penalty term}

\item{outcome}{matrix of 'dummied' outcomes}

\item{a}{template list of activation matrices (with "zeroth" layer filled in with inputs)}
}
\description{
backProp performs a forward propogation with current values
of Thetas, then performs a backward propogation to calculate
gradient needed for optimisation
}

